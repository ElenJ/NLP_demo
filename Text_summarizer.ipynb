{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text summarization using Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script demonstartes the usage of the Hugging Face Transformers library to perform text summarization using a pre-trained model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38981,
     "status": "ok",
     "timestamp": 1739013261118,
     "user": {
      "displayName": "Elena Jolkver",
      "userId": "14129354198230546329"
     },
     "user_tz": -60
    },
    "id": "g_ZApTeYsfsz",
    "outputId": "b0c5ffd1-8821-4ffd-a27c-21100c948f05"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\elena.jolkver\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk  # Import the Natural Language Toolkit library\n",
    "nltk.download(\"punkt\")  # Download the 'punkt' tokenizer models from NLTK\n",
    "from nltk.tokenize import sent_tokenize  # Import the sentence tokenizer from NLTK\n",
    "\n",
    "from transformers import pipeline, set_seed  # Import the pipeline and set_seed functions from the transformers library\n",
    "\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser  # Import PlainTextParser from the sumy package\n",
    "from sumy.nlp.tokenizers import Tokenizer  # Import Tokenizer from the sumy package\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer  # Import TextRankSummarizer from the sumy package\n",
    "\n",
    "from datasets import load_dataset  # Import functions to load datasets from the datasets library\n",
    "\n",
    "import evaluate # Import the evaluate library for model evaluation\n",
    "\n",
    "import pandas as pd  # Import the pandas library to work with tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nltk (Natural Language Toolkit) is a comprehensive library for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and more. The 'punkt' tokenizer specifically helps in sentence tokenization, which is the process of dividing a text into a list of its component sentences. \n",
    "\n",
    "The transformers library by Huggingface is a state-of-the-art natural language processing library that provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio (Hugging Face – The AI Community Building the Future., 2025). The 'pipeline' function simplifies the implementation of complex models for various NLP tasks, including text generation and summarization.\n",
    "\n",
    "Sumy is a Python library used for text summarization. It offers several algorithms for extracting summaries from text documents. The PlaintextParser is used to read and parse plain text files. TextRankSummarizer is an implementation of the TextRank algorithm, a graph-based summarization technique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 16001 examples [02:08, 124.18 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mccdv/cnn_dailymail\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m3.0.0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain[:1]\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Access the first article\u001b[39;00m\n\u001b[32m      4\u001b[39m first_article = dataset[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elena.jolkver\\AppData\\Local\\miniforge3\\envs\\nlp\\Lib\\site-packages\\datasets\\load.py:2084\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2081\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance.as_streaming_dataset(split=split)\n\u001b[32m   2083\u001b[39m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2084\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2086\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2090\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2092\u001b[39m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[32m   2093\u001b[39m keep_in_memory = (\n\u001b[32m   2094\u001b[39m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance.info.dataset_size)\n\u001b[32m   2095\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elena.jolkver\\AppData\\Local\\miniforge3\\envs\\nlp\\Lib\\site-packages\\datasets\\builder.py:925\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    924\u001b[39m     prepare_split_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_proc\u001b[39m\u001b[33m\"\u001b[39m] = num_proc\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[32m    932\u001b[39m \u001b[38;5;28mself\u001b[39m.info.dataset_size = \u001b[38;5;28msum\u001b[39m(split.num_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info.splits.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elena.jolkver\\AppData\\Local\\miniforge3\\envs\\nlp\\Lib\\site-packages\\datasets\\builder.py:1649\u001b[39m, in \u001b[36mGeneratorBasedBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[39m\n\u001b[32m   1648\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, **prepare_splits_kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1649\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_duplicate_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBASIC_CHECKS\u001b[49m\n\u001b[32m   1653\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mALL_CHECKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1654\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_splits_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1655\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elena.jolkver\\AppData\\Local\\miniforge3\\envs\\nlp\\Lib\\site-packages\\datasets\\builder.py:1001\u001b[39m, in \u001b[36mDatasetBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[39m\n\u001b[32m    997\u001b[39m split_dict.add(split_generator.split_info)\n\u001b[32m    999\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1000\u001b[39m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1003\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m   1004\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot find data file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1005\u001b[39m         + (\u001b[38;5;28mself\u001b[39m.manual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1006\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1007\u001b[39m         + \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[32m   1008\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elena.jolkver\\AppData\\Local\\miniforge3\\envs\\nlp\\Lib\\site-packages\\datasets\\builder.py:1487\u001b[39m, in \u001b[36mGeneratorBasedBuilder._prepare_split\u001b[39m\u001b[34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[39m\n\u001b[32m   1485\u001b[39m job_id = \u001b[32m0\u001b[39m\n\u001b[32m   1486\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m1487\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_prepare_split_args\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elena.jolkver\\AppData\\Local\\miniforge3\\envs\\nlp\\Lib\\site-packages\\datasets\\builder.py:1608\u001b[39m, in \u001b[36mGeneratorBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[39m\n\u001b[32m   1606\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1607\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m1608\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1609\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_num_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1610\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\ccdv--cnn_dailymail\\0107f7388b5c6fae455a5661bcd134fc22da53ea75852027040d8d1e997f101f\\cnn_dailymail.py:272\u001b[39m, in \u001b[36mCnnDailymail._generate_examples\u001b[39m\u001b[34m(self, files)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_examples\u001b[39m(\u001b[38;5;28mself\u001b[39m, files):\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m         article, highlights = \u001b[43m_get_art_abs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m article \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m highlights:\n\u001b[32m    274\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\ccdv--cnn_dailymail\\0107f7388b5c6fae455a5661bcd134fc22da53ea75852027040d8d1e997f101f\\cnn_dailymail.py:181\u001b[39m, in \u001b[36m_get_art_abs\u001b[39m\u001b[34m(story_file, tfds_version)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get abstract (highlights) and article from a story file path.\"\"\"\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# Based on https://github.com/abisee/cnn-dailymail/blob/master/\u001b[39;00m\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m#     make_datafiles.py\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m lines = \u001b[43m_read_text_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstory_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# The github code lowercase the text and we removed it in 3.0.0.\u001b[39;00m\n\u001b[32m    184\u001b[39m \n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# Put periods on the ends of lines that are missing them\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# (this is a problem in the dataset because many image captions don't end in\u001b[39;00m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# periods; consequently they end up in the body of the article as run-on\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# sentences)\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfix_missing_period\u001b[39m(line):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\ccdv--cnn_dailymail\\0107f7388b5c6fae455a5661bcd134fc22da53ea75852027040d8d1e997f101f\\cnn_dailymail.py:170\u001b[39m, in \u001b[36m_read_text_file\u001b[39m\u001b[34m(text_file)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_text_file\u001b[39m(text_file):\n\u001b[32m    169\u001b[39m     lines = []\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    171\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[32m    172\u001b[39m             lines.append(line.strip())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elena.jolkver\\AppData\\Local\\miniforge3\\envs\\nlp\\Lib\\site-packages\\datasets\\streaming.py:75\u001b[39m, in \u001b[36mextend_module_for_streaming.<locals>.wrap_auth.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elena.jolkver\\AppData\\Local\\miniforge3\\envs\\nlp\\Lib\\site-packages\\datasets\\utils\\file_utils.py:944\u001b[39m, in \u001b[36mxopen\u001b[39m\u001b[34m(file, mode, download_config, *args, **kwargs)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_path(main_hop):\n\u001b[32m    942\u001b[39m     \u001b[38;5;66;03m# ignore fsspec-specific kwargs\u001b[39;00m\n\u001b[32m    943\u001b[39m     kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mblock_size\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmain_hop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# add headers and cookies for authentication on the HF Hub and for Google Drive\u001b[39;00m\n\u001b[32m    946\u001b[39m file, storage_options = _prepare_path_and_storage_options(file_str, download_config=download_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:309\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, errors)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('ccdv/cnn_dailymail', '3.0.0', split='train[:1]', trust_remote_code = True)\n",
    "\n",
    "# Access the first article\n",
    "first_article = dataset[0]\n",
    "print(first_article['article'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have selected the CNN/DailyMail dataset (Hermann et al., 2015) for our analysis of summarization models. This dataset includes approximately 300,000 pairs of news articles and their corresponding summaries. These summaries are derived from the bullet points that CNN and the DailyMail add to their articles. A notable feature of this dataset is that the summaries are not mere excerpts but are instead abstractive, generating new sentences that encapsulate the essence of the articles.\n",
    "\n",
    "For illustrative purposes, we will apply our summarization techniques to a single article. Lengthy articles present a significant challenge to most transformer models due to their typical context size limitation of about 1,000 tokens, which equates to several paragraphs. A common, albeit rudimentary, strategy to manage this constraint is to truncate texts that exceed the model's context size. Although crucial content might reside towards the end of the text, we must navigate this limitation of the model architectures for now, thereby limiting the article to the first 2000 characters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWKyhoFusbKp"
   },
   "outputs": [],
   "source": [
    "sample_text = dataset[\"train\"][1][\"article\"][:2000]  # Get the first 2000 characters of the second article in the training dataset\n",
    "sample_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the absence of human-generated \"true\" summaries within the text corpus, we can create a reference point by establishing a simple baseline. This baseline allows us to compare the machine-generated summaries against a consistent benchmark. To achieve this, we extract the first three sentences of the text to serve as our baseline summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zwOzPr2soOx"
   },
   "outputs": [],
   "source": [
    "# We'll collect the generated summaries of each model in a dictionary\n",
    "summaries = {}  # Initialize an empty dictionary to store summaries\n",
    "\n",
    "def three_sentence_summary(text):  # Define a function to generate a three-sentence summary\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])  # Return the first three sentences of the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G07n331BsppB"
   },
   "outputs": [],
   "source": [
    "summaries[\"baseline\"] = three_sentence_summary(sample_text)  # Generate a baseline summary and store it in the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline summary, extracted as the first three sentences of the article, already provides a concise encapsulation of the key events: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1739013262678,
     "user": {
      "displayName": "Elena Jolkver",
      "userId": "14129354198230546329"
     },
     "user_tz": -60
    },
    "id": "3254wRhvtPj_",
    "outputId": "ab01abb8-b435-45da-a634-8002d145b588"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay.\\nThe fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds.\\nThe U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[\"baseline\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed with generating a summary using the classical algorithm provided by the Sumy library. One of the key algorithms provided by Sumy is the TextRankSummarizer, which implements the TextRank algorithm. TextRank is a classical unsupervised algorithm inspired by Google's PageRank method used for ranking web pages. As it doesn’t require pre-labeled data or training, it is versatile option when large annotated datasets aren't available. Its effectiveness in summarizing text, especially when domain-specific training data is scarce, makes it a valuable tool in the toolkit of text summarization techniques. By using the TextRankSummarizer from the Sumy library, we can efficiently generate summaries that aim to capture the essential information and main points of the source documents without needing extensive computational resources or training data.\n",
    "Here's how TextRank operates in the context of text summarization:\n",
    "\n",
    "1. Graph-Based Approach: TextRank constructs a graph where sentences are nodes. The edges between sentences are weighted based on the similarity between sentences, typically computed using measures like cosine similarity on word vectors.\n",
    "\n",
    "2. Sentence Representation: Sentences in the text are represented as nodes in the graph, and the algorithm establishes connections (edges) between these nodes based on semantic similarity; the more similar two sentences are, the stronger the connection.\n",
    "\n",
    "3. Ranking Sentences: Once the graph is built, the TextRank algorithm applies a ranking process similar to PageRank to score the nodes (sentences). This process iteratively refines the score of each sentence based on its connections and the score of the connected sentences.\n",
    "\n",
    "4. Extracting Key Sentences: After the ranking process converges, the sentences with the highest scores are extracted as the summary. Typically, the top-ranked sentences are selected to form a coherent and concise summary of the original text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1739013624188,
     "user": {
      "displayName": "Elena Jolkver",
      "userId": "14129354198230546329"
     },
     "user_tz": -60
    },
    "id": "J7F9HFWGFQkS",
    "outputId": "b7c51487-d58a-438c-ff01-2ef487474ee3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay.\n",
      "The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds.\n",
      "The 26-year-old Bolt has now collected eight gold medals at world championships, equaling the record held by American trio Carl Lewis, Michael Johnson and Allyson Felix, not to mention the small matter of six Olympic titles.\n",
      "The relay triumph followed individual successes in the 100 and 200 meters in the Russian capital.\n",
      "Defending champions, the United States, were initially back in the bronze medal position after losing time on the second handover between Alexandria Anderson and English Gardner, but promoted to silver when France were subsequently disqualified for an illegal handover.\n"
     ]
    }
   ],
   "source": [
    "parser = PlaintextParser.from_string(sample_text, tokenizer = Tokenizer(\"english\"))  # Use PlainTextParser to parse the sample text\n",
    "summarizer = TextRankSummarizer()  # Initialize the TextRank summarizer\n",
    "\n",
    "# Collect the summary sentences in a list\n",
    "summary_sentences = []  # Initialize an empty list to store summary sentences\n",
    "for sentence in summarizer(parser.document, 5):  # Generate summary sentences using the TextRank summarizer\n",
    "    summary_sentences.append(str(sentence))  # Append each summary sentence to the list\n",
    "\n",
    "# Join the sentences to form a single summary string\n",
    "summaries[\"sumy\"] = \"\\n\".join(summary_sentences)  # Store the generated summary in the dictionary\n",
    "\n",
    "# Print the summary (optional)\n",
    "print(summaries[\"sumy\"])  # Print the summary generated by the TextRank summarizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final step, we employ three different large language models (LLMs) for summarization: GPT, BART, and a lightweight version of DeepSeek. Among these, GPT-2 and DeepSeek serve as general-purpose LLMs. We prompt these models by appending \"TL;DR:\" to the article, which is shorthand for \"too long; didn’t read\" and commonly signals a brief summary. Language models recognize this due to its frequent occurrence in training data, interpreting it as an instruction to summarize. This allows for conditional generation, where the model, given this prefix, produces the subsequent words, forming the summary. In contrast, the specific BART model employed here has been fine-tuned on the CNN/DailyMail dataset for summarization tasks. Consequently, BART acts as a form of positive control in our experiment, given its prior training on the very dataset we are utilizing for demonstration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112105,
     "status": "ok",
     "timestamp": 1739013374783,
     "user": {
      "displayName": "Elena Jolkver",
      "userId": "14129354198230546329"
     },
     "user_tz": -60
    },
    "id": "EM5YqA56sqSj",
    "outputId": "aa249fcf-fc05-4f6a-fe88-27278ba7896b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# GPT2 summary\n",
    "set_seed(42)  # Set the random seed for reproducibility\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2-xl\")  # Initialize a text generation pipeline with the GPT-2 XL model\n",
    "query = sample_text + \"\\nTL;DR:\\n\"  # Create a query for the GPT-2 model\n",
    "pipe_out = pipe(query, max_new_tokens=1000, clean_up_tokenization_spaces=True)  # Generate text using the GPT-2 model\n",
    "summaries[\"gpt2\"] = \"\\n\".join(  # Store the generated summary in the dictionary\n",
    "    sent_tokenize(pipe_out[0][\"generated_text\"][len(query) :]))  # Tokenize the generated text into sentences\n",
    "del pipe\n",
    "del pipe_out\n",
    "del query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "executionInfo": {
     "elapsed": 249,
     "status": "error",
     "timestamp": 1742538293077,
     "user": {
      "displayName": "Elena Jolkver",
      "userId": "14129354198230546329"
     },
     "user_tz": -60
    },
    "id": "Ie6s-VIrB8aA",
    "outputId": "9a345626-03c7-4cd4-86ed-248f3f0bdc3e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'set_seed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c397c32ebec1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# DeepSeek Summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\u001b[0m  \u001b[0;31m# Define the model name for the DeepSeek model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set the random seed for reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Initialize a text generation pipeline with the DeepSeek model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\nTL;DR:\\n\"\u001b[0m  \u001b[0;31m# Create a query for the DeepSeek model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'set_seed' is not defined"
     ]
    }
   ],
   "source": [
    "# DeepSeek Summary\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"  # Define the model name for the DeepSeek model\n",
    "set_seed(42)  # Set the random seed for reproducibility\n",
    "pipe = pipeline(\"text-generation\", model=model_name)  # Initialize a text generation pipeline with the DeepSeek model\n",
    "query = sample_text + \"\\nTL;DR:\\n\"  # Create a query for the DeepSeek model\n",
    "pipe_out = pipe(query, max_new_tokens=1000, clean_up_tokenization_spaces=True)  # Generate text using the DeepSeek model\n",
    "summaries[\"DeepSeek\"] = \"\\n\".join(  # Store the generated summary in the dictionary\n",
    "    sent_tokenize(pipe_out[0][\"generated_text\"][len(query) :]))  # Tokenize the generated text into sentences\n",
    "del pipe\n",
    "del pipe_out\n",
    "del query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "executionInfo": {
     "elapsed": 36248,
     "status": "error",
     "timestamp": 1739013595904,
     "user": {
      "displayName": "Elena Jolkver",
      "userId": "14129354198230546329"
     },
     "user_tz": -60
    },
    "id": "aKP2eSEUsyno",
    "outputId": "ab97587a-1ef4-4ec9-e6ac-f25283580981"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8f8873803f96>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mpipe_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "#BART summary\n",
    "pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")  # Initialize a summarization pipeline with the BART model\n",
    "pipe_out = pipe(sample_text)  # Generate a summary using the BART model\n",
    "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))  # Store the generated summary in the dictionary\n",
    "del pipe\n",
    "del pipe_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1739013625981,
     "user": {
      "displayName": "Elena Jolkver",
      "userId": "14129354198230546329"
     },
     "user_tz": -60
    },
    "id": "VTYBvLdhG2yP",
    "outputId": "a953f0b1-1620-4d52-8786-fbe2dd76c526"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"In the men's 4x100 relay, Usain Bolt secured his third gold medal in Moscow, defeating Justin Gatlin, resulting in Jamaica's victory.\\nThe U.S. team took second, while Canada and Britain finished third and fourth, respectively.\\nThe relay was won by Bolt with Ashmeade providing the baton, and Bolt successfully took control of the baton from Gatlin.\\nThe individual performances in the sprint events also contributed to Jamaica's dominance.\\n</think>\\n\\nIn the men's 4x100m relay, Usain Bolt secured his third gold medal in Moscow, defeating Justin Gatlin, resulting in Jamaica's victory.\\nThe U.S. team took second, while Canada and Britain finished third and fourth, respectively.\\nThe relay was won by Bolt with Ashmeade providing the baton, and Bolt successfully took control of the baton from Gatlin.\\nThe individual performances in the sprint events also contributed to Jamaica's dominance.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[\"DeepSeek\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOoUqmV4Iwdq"
   },
   "source": [
    "Comparing summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1739013632859,
     "user": {
      "displayName": "Elena Jolkver",
      "userId": "14129354198230546329"
     },
     "user_tz": -60
    },
    "id": "UOPwW_wqIvld",
    "outputId": "3f3f715a-7aa9-43e7-f28e-81da848f7098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL TAXT\n",
      "(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay.\n",
      "The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds.\n",
      "The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover.\n",
      "The 26-year-old Bolt has now collected eight gold medals at world championships, equaling the record held by American trio Carl Lewis, Michael Johnson and Allyson Felix, not to mention the small matter of six Olympic titles.\n",
      "The relay triumph followed individual successes in the 100 and 200 meters in the Russian capital.\n",
      "\"I'm proud of myself and I'll continue to work to dominate for as long as possible,\" Bolt said, having previously expressed his intention to carry on until the 2016 Rio Olympics.\n",
      "Victory was never seriously in doubt once he got the baton safely in hand from Ashmeade, while Gatlin and the United States third leg runner Rakieem Salaam had problems.\n",
      "Gatlin strayed out of his lane as he struggled to get full control of their baton and was never able to get on terms with Bolt.\n",
      "Earlier, Jamaica's women underlined their dominance in the sprint events by winning the 4x100m relay gold, anchored by Shelly-Ann Fraser-Pryce, who like Bolt was completing a triple.\n",
      "Their quartet recorded a championship record of 41.29 seconds, well clear of France, who crossed the line in second place in 42.73 seconds.\n",
      "Defending champions, the United States, were initially back in the bronze medal position after losing time on the second handover between Alexandria Anderson and English Gardner, but promoted to silver when France were subsequently disqualified for an illegal handover.\n",
      "The British quartet, who were initially fourth, were promoted to the bronze which eluded their men's team.\n",
      "Fraser-Pryce, like Bolt ag\n",
      "\n",
      "HUMAN SUMMARY\n",
      "Usain Bolt wins third gold of world championship .\n",
      "Anchors Jamaica to 4x100m relay victory .\n",
      "Eighth gold at the championships for Bolt .\n",
      "Jamaica double up in women's 4x100m relay .\n",
      "\n",
      "BASELINE\n",
      "(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay.\n",
      "The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds.\n",
      "The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover.\n",
      "\n",
      "GPT2\n",
      "Bolt finished in style by winning the final sprint event in Moscow.\n",
      "There is still a few sprint events to go before Friday's final but so far this final gold should stand as a world record.\n",
      "\n",
      "DEEPSEEK\n",
      "In the men's 4x100 relay, Usain Bolt secured his third gold medal in Moscow, defeating Justin Gatlin, resulting in Jamaica's victory.\n",
      "The U.S. team took second, while Canada and Britain finished third and fourth, respectively.\n",
      "The relay was won by Bolt with Ashmeade providing the baton, and Bolt successfully took control of the baton from Gatlin.\n",
      "The individual performances in the sprint events also contributed to Jamaica's dominance.\n",
      "</think>\n",
      "\n",
      "In the men's 4x100m relay, Usain Bolt secured his third gold medal in Moscow, defeating Justin Gatlin, resulting in Jamaica's victory.\n",
      "The U.S. team took second, while Canada and Britain finished third and fourth, respectively.\n",
      "The relay was won by Bolt with Ashmeade providing the baton, and Bolt successfully took control of the baton from Gatlin.\n",
      "The individual performances in the sprint events also contributed to Jamaica's dominance.\n",
      "\n",
      "BART\n",
      "Usain Bolt wins his third gold of the world championships in Moscow.\n",
      "Bolt anchors Jamaica to victory in the men's 4x100m relay.\n",
      "The 26-year-old has now won eight gold medals at world championships.\n",
      "Jamaica's women also win gold in the relay, beating France in the process.\n",
      "\n",
      "SUMY\n",
      "(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay.\n",
      "The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds.\n",
      "The 26-year-old Bolt has now collected eight gold medals at world championships, equaling the record held by American trio Carl Lewis, Michael Johnson and Allyson Felix, not to mention the small matter of six Olympic titles.\n",
      "The relay triumph followed individual successes in the 100 and 200 meters in the Russian capital.\n",
      "Defending champions, the United States, were initially back in the bronze medal position after losing time on the second handover between Alexandria Anderson and English Gardner, but promoted to silver when France were subsequently disqualified for an illegal handover.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL TEXT\")\n",
    "for sentence in sent_tokenize(sample_text):\n",
    "    print(sentence)\n",
    "print(\"\")\n",
    "\n",
    "print(\"HUMAN SUMMARY\")\n",
    "print(dataset[\"train\"][1][\"highlights\"])\n",
    "print(\"\")\n",
    "\n",
    "for model_name in summaries:\n",
    "    print(model_name.upper())\n",
    "    print(summaries[model_name])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But which of these summaries comes closest to the reference summary? To this end, we evaluate the results with the ROUGE metric set, which measures the overlap of n-grams between the machine-generated and reference summaries.\n",
    "\n",
    "- ROUGE-1 measures the overlap of unigrams (single words) between the generated and reference summary , implying the generated summary captures more key terms from the reference.\n",
    "\n",
    "- ROUGE-2 assesses the overlap of bigrams (two-word sequences), suggesting more phrase-level fidelity and detail in the generated summary relative to the reference.\n",
    "\n",
    "- ROUGE-L considers the longest common subsequence, highlighting fluency and coherence.\n",
    "\n",
    "- ROUGE-Lsum is a variant of ROUGE-L specifically tuned for summarization tasks, assessing sentence splits, a higher score suggests a better overall structural and sentence-level match to the reference summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvY7-EuHI1x-"
   },
   "outputs": [],
   "source": [
    "rouge_metric = load_metric(\"rouge\")  # Load the ROUGE metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1325,
     "status": "ok",
     "timestamp": 1739013657012,
     "user": {
      "displayName": "Elena Jolkver",
      "userId": "14129354198230546329"
     },
     "user_tz": -60
    },
    "id": "w5nO43knI54M",
    "outputId": "c971aa82-31e3-4213-ae15-5dd32c3ffbd5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"rouge1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1619861291688594,\n        \"min\": 0.1954022988505747,\n        \"max\": 0.5822784810126583,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.21212121212121213,\n          0.21857923497267756,\n          0.1954022988505747\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07839629450674626,\n        \"min\": 0.0,\n        \"max\": 0.20779220779220778,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0,\n          0.055248618784530384,\n          0.046511627906976744\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1396965689588945,\n        \"min\": 0.12121212121212122,\n        \"max\": 0.45569620253164556,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.12121212121212122,\n          0.13114754098360656,\n          0.14942528735632185\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeLsum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13866241782963082,\n        \"min\": 0.1724137931034483,\n        \"max\": 0.5063291139240508,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.21212121212121213,\n          0.18579234972677597,\n          0.1724137931034483\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-d6dbc8c0-cbb2-4826-a4f4-4f4d784d0bff\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.232143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2</th>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.212121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepSeek</th>\n",
       "      <td>0.195402</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.149425</td>\n",
       "      <td>0.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bart</th>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.207792</td>\n",
       "      <td>0.455696</td>\n",
       "      <td>0.506329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sumy</th>\n",
       "      <td>0.218579</td>\n",
       "      <td>0.055249</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.185792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6dbc8c0-cbb2-4826-a4f4-4f4d784d0bff')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-d6dbc8c0-cbb2-4826-a4f4-4f4d784d0bff button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-d6dbc8c0-cbb2-4826-a4f4-4f4d784d0bff');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-82c7a39a-118d-442e-9b1b-0fc03cefd1b9\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-82c7a39a-118d-442e-9b1b-0fc03cefd1b9')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-82c7a39a-118d-442e-9b1b-0fc03cefd1b9 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "            rouge1    rouge2    rougeL  rougeLsum\n",
       "baseline  0.303571  0.090909  0.214286   0.232143\n",
       "gpt2      0.212121  0.000000  0.121212   0.212121\n",
       "DeepSeek  0.195402  0.046512  0.149425   0.172414\n",
       "bart      0.582278  0.207792  0.455696   0.506329\n",
       "sumy      0.218579  0.055249  0.131148   0.185792"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference = dataset[\"train\"][1][\"highlights\"]  # Get the reference summary from the dataset\n",
    "records = []  # Initialize an empty list to store ROUGE scores\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]  # Define the names of the ROUGE metrics\n",
    "\n",
    "for model_name in summaries:  # Iterate over the generated summaries\n",
    "    rouge_metric.add(prediction=summaries[model_name], reference=reference)  # Add the prediction and reference to the ROUGE metric\n",
    "    score = rouge_metric.compute()  # Compute the ROUGE scores\n",
    "    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)  # Create a dictionary of ROUGE scores\n",
    "    records.append(rouge_dict)  # Append the ROUGE scores to the list\n",
    "\n",
    "pd.DataFrame.from_records(records, index=summaries.keys())  # Create a DataFrame from the ROUGE scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results we can make several conclusions: \n",
    "\n",
    "1. Baseline (Three-Sentence Summary): ROUGE-1: 0.30 and ROUGE-L: 0.21 indicate that while the baseline method captures some key words from the reference summary, it lacks depth in fluency (ROUGE-L shows limited coherence). ROUGE-2: 0.09 is relatively low, suggesting a lack of capturing phrase-level information.\n",
    "\n",
    "2. GPT-2: ROUGE-1: 0.21 shows reduced unigram overlap compared to the baseline, which might mean GPT-2 generated broader text rather than sticking to precision words. ROUGE-2: 0.00 indicates GPT-2 did not capture any bigram matches, suggesting generated summaries missed critical phrase-level consistency. ROUGE-L: 0.12 suggests lesser fluency and coherence than the baseline.\n",
    "\n",
    "3. DeepSeek: ROUGE-1: 0.19 and ROUGE-2: 0.04, both lower than the baseline, showing minimal key content and phrase overlap.\n",
    "\n",
    "4. BART: ROUGE-1: 0.58 and ROUGE-2: 0.20 are significantly higher than other models, highlighting BART’s effectiveness in retaining both key terms and phrase-level details. ROUGE-L: 0.45 and ROUGE-Lsum: 0.50 demonstrate strong fluency, coherence, and effectiveness in summarizing content akin to the reference.\n",
    "\n",
    "5. Sumy (TextRank): ROUGE-1: 0.21 and ROUGE-2: 0.05 slightly outperform GPT-2 and DeepSeek, indicating better capture of key terms and some phrases. ROUGE-L: 0.13 suggests moderate coherence.\n",
    "\n",
    "BART performs best across all ROUGE metrics, which is not surprising given that it has been pre-trained specifically on the CNN/DailyMail dataset. This pre-training allows BART to generate summaries with extensive content overlap and coherence when compared to the reference summaries. However, its high performance should be viewed in the context of its familiarity with the dataset, suggesting that BART may be particularly well-suited for tasks involving similar data but may not generalize as well to entirely new datasets without further fine-tuning. Furthermore we see that GPT-2 and (a slim version of) DeepSeek perform comparably, while the classical method is performing in a comparable range as LLM models. \n",
    "\n",
    "In summary, the performance analysis of different summarization approaches highlights the importance of model-data alignment in achieving effective results. BART’s remarkable performance underscores the advantages of using models fine-tuned on specific datasets, though with the caveat of potentially limited generalizability. Meanwhile, GPT-2 and DeepSeek illustrate the capabilities of general LLMs, and the classical TextRank approach remains a viable option, especially in contexts lacking expansive computational resources or annotated data. As you consider implementing these techniques, balance model selection with the specific needs and constraints of your intended application, and be mindful of the potential need for fine-tuning to optimize performance across diverse datasets.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMMHeKloNKwxF6Vr2CcSbjA",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2c2ff65b8df546119d4029b3baa3f453": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39a581e3211742f0bb0d8891a9b01016",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5883fcecee1940ea81691f658bd8ac57",
      "value": 3
     }
    },
    "39a581e3211742f0bb0d8891a9b01016": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43975478fdb647b981fbbc440a5a69e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "48001e71bd0a4d03b9da1327044f6c42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_58c43a5d4681455290d39a7ac9f80815",
       "IPY_MODEL_2c2ff65b8df546119d4029b3baa3f453",
       "IPY_MODEL_9adc5a363b6d46d9a57639593d12719b"
      ],
      "layout": "IPY_MODEL_c9b9d2f8af4d413e9518c647ae23a7e3"
     }
    },
    "5883fcecee1940ea81691f658bd8ac57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "58c43a5d4681455290d39a7ac9f80815": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_86a84b77ef5b48cd9518aafa79917b05",
      "placeholder": "​",
      "style": "IPY_MODEL_43975478fdb647b981fbbc440a5a69e9",
      "value": "100%"
     }
    },
    "86a84b77ef5b48cd9518aafa79917b05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a39549e9525486f95fce4c8b2fdb59a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9adc5a363b6d46d9a57639593d12719b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da450e36bac9437483bce0d73bdd63cd",
      "placeholder": "​",
      "style": "IPY_MODEL_8a39549e9525486f95fce4c8b2fdb59a",
      "value": " 3/3 [00:00&lt;00:00,  2.14it/s]"
     }
    },
    "c9b9d2f8af4d413e9518c647ae23a7e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da450e36bac9437483bce0d73bdd63cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
